{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLtk51E6UjTm"
      },
      "source": [
        "# Lab 4 - Information Theory in Machine Learning\n",
        "\n",
        "Welcome to this week's lab on Information Theory! This week, we will dive into the fascinating world of Information Theory as applied to Machine Learning. Specifically, we will focus on two key concepts: Entropy and Information Gain. These principles are fundamental in understanding how decision trees make split decisions to organize data effectively.\n",
        "\n",
        "### Entropy\n",
        "- Entropy, in the context of information theory, measures the level of uncertainty or disorder within a set of data.\n",
        "- In machine learning, particularly in decision trees, entropy helps to determine how a dataset should be split. A high entropy means more disorder, indicating that our dataset is varied. Conversely, low entropy suggests more uniformity in the data.\n",
        "\n",
        "### Information Gain\n",
        "- Information Gain measures the reduction in entropy after the dataset is split on an attribute.\n",
        "- It is crucial in building decision trees as it helps to decide the order of attributes the tree will use for splitting the data. The attribute with the highest Information Gain is chosen as the splitting attribute at each node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sSQRUYZUjTp"
      },
      "source": [
        "## Part 1: Entropy and Information Gain in Decision Trees\n",
        "Decision Trees use these concepts to create branches. By choosing splits that maximize Information Gain (or equivalently minimize entropy), a decision tree can effectively categorize data, leading to better classification or regression models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kFKlzPcUjTp"
      },
      "source": [
        "### Step 1: Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qIaqHeMUUjTq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3n36efWUjTr"
      },
      "source": [
        "### Step 2: Load and Explore the Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "80qgHlX8UjTr"
      },
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOEVg3URUjTr"
      },
      "source": [
        "### Step 3: Calculate Entropy\n",
        "To calculate the `entropy` we need to:\n",
        "- First, extract the target variable `y` from your dataset (like the 'target' column in the Iris dataset).\n",
        "- Then, call `calculate_entropy(y)` to get the entropy.\n",
        "\n",
        "This function calculates the entropy of a given target variable `y`. It works by first determining the unique classes in `y`, then computes the probability of each class, and uses this probability to calculate the entropy. This is a crucial step in understanding the disorder or uncertainty in the dataset, a fundamental concept in information theory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h819kargUjTs"
      },
      "outputs": [],
      "source": [
        "def calculate_entropy(y):\n",
        "    class_labels = np.unique(y)\n",
        "    entropy = 0\n",
        "    for label in class_labels:\n",
        "        probability = len(y[y == label]) / len(y)\n",
        "        entropy -= probability * np.log2(probability)\n",
        "    return entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVz-lwiRUjTs"
      },
      "source": [
        "Calculate the entropy for the target variable.  What is your observastion about the calculated Entropy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0iw7voBUjTs",
        "outputId": "34b426f8-ff68-4433-efb5-b95bbb30f18c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of the target variable: 1.584962500721156\n"
          ]
        }
      ],
      "source": [
        "target_entropy = calculate_entropy(df['target'])\n",
        "print(f\"Entropy of the target variable: {target_entropy}\")\n",
        "\n",
        "#The minimal target value suggests the target variable belongs to a highly uncertain category that matches the expectations for balanced 3-class data. The evaluation indicates that the data set will prove suitable for applying both decision trees and other classification models for training purposes.The available data contains enough information to divide it into useful classification groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCc_1lhTUjTt"
      },
      "source": [
        "### Step 4: Calculate Information Gain\n",
        "There are three steps for calculating the Information Gain:\n",
        "1. Compute Overall Entropy: Use the entropy function from Step 3 on the entire target dataset.\n",
        "2. Calculate Weighted Entropy for Each Attribute: For each unique value in the attribute, partition the dataset and calculate its entropy. Then calculate the weighted sum of these entropies, where the weights are the proportions of instances in each partition.\n",
        "3. Compute Information Gain: Subtract the weighted entropy of the split from the original entropy.\n",
        "\n",
        "The attribute with the highest Information Gain is generally chosen for splitting, as it provides the most significant reduction in uncertainty. This step is critical in constructing an effective decision tree, as it directly influences the structure and depth of the tree.\n",
        "\n",
        "**Use the provided function to calculate the information gain for each of the features in the dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7aR3-bSQUjTt"
      },
      "outputs": [],
      "source": [
        "def calculate_information_gain(df, attribute, target_name):\n",
        "    total_entropy = calculate_entropy(df[target_name])\n",
        "    values, counts = np.unique(df[attribute], return_counts=True)\n",
        "    weighted_entropy = sum((counts[i] / sum(counts)) * calculate_entropy(df.where(df[attribute] == values[i]).dropna()[target_name]) for i in range(len(values)))\n",
        "    information_gain = total_entropy - weighted_entropy\n",
        "    return information_gain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ANm7MzOUjTu"
      },
      "source": [
        "Discuss your findings here."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The overall entropy computation requires applying the Step 3 entropy function to the entire target dataset.\n",
        "2. Wetted Entropy values need calculation for each attribute through partitioning the dataset before conducting the entropy measurement. The individual partition entropies are combined into one weighted measure by using partition proportions as weights.\n",
        "3. Check Information Gain by subtracting weighted entropy of the split from original entropy.\n",
        "\n",
        "Information Gain reaches its maximum value when selecting splitting features since this approach minimizes unpredictability most effectively. Therefore, the chosen feature results in the biggest uncertainty reduction. The determination of a split criterion during this step plays a central role in building a proper decision tree because it both influences its overall shape and its depth."
      ],
      "metadata": {
        "id": "qxeH1fmpZb9K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCCUh1s_UjTu"
      },
      "source": [
        "## Part 2: Apply Entropy and Information Gain on a different dataset\n",
        "\n",
        "Your task is to choose a new dataset and implement what you learned in `Part 1` on this new dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-niS6qSUjTu"
      },
      "source": [
        "### Task 1: Implement Entropy and Information Gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq_f1PeQUjTu",
        "outputId": "0ccbaade-3b75-4c6e-cc46-1e62ddff7e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Workout Duration (mins):\n",
            "  Max Information Gain = 6.5879\n",
            "  Best Split Value = N/A\n",
            "\n",
            "Calories Burned:\n",
            "  Max Information Gain = 8.9658\n",
            "  Best Split Value = N/A\n",
            "\n",
            "Step Count:\n",
            "  Max Information Gain = 8.9418\n",
            "  Best Split Value = N/A\n",
            "\n",
            "Heart Rate:\n",
            "  Max Information Gain = 6.7247\n",
            "  Best Split Value = N/A\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"fitness_tracker.csv\")\n",
        "\n",
        "def entropy(y):\n",
        "    \"\"\"Calculates the entropy of a dataset.\"\"\"\n",
        "    class_counts = Counter(y)\n",
        "    total_samples = len(y)\n",
        "    entropy_value = 0.0\n",
        "\n",
        "    for count in class_counts.values():\n",
        "        probability = count / total_samples\n",
        "        entropy_value -= probability * np.log2(probability)\n",
        "\n",
        "    return entropy_value\n",
        "\n",
        "def information_gain(df, attribute, target_name):\n",
        "    #Calculates the information gain of an attribute\n",
        "\n",
        "    total_entropy = entropy(df[target_name])\n",
        "    values, counts = np.unique(df[attribute], return_counts=True)\n",
        "    weighted_entropy = sum((counts[i] / sum(counts)) * entropy(df.where(df[attribute] == values[i]).dropna()[target_name]) for i in range(len(values)))\n",
        "    information_gain = total_entropy - weighted_entropy\n",
        "    return information_gain, None  # Returning None for split, as it's not calculated here\n",
        "\n",
        "target = 'ID'\n",
        "features = ['Workout Duration (mins)', 'Calories Burned', 'Step Count', 'Heart Rate']\n",
        "\n",
        "# Calculate information gain for each feature\n",
        "ig_results = {}\n",
        "for feature in features:\n",
        "    gain, split = information_gain(df, feature, target)\n",
        "    ig_results[feature] = (gain, split)\n",
        "\n",
        "# Display results\n",
        "for feature, (gain, split) in ig_results.items():\n",
        "    print(f\"{feature}:\")\n",
        "    print(f\"  Max Information Gain = {gain:.4f}\")\n",
        "    print(f\"  Best Split Value = {split if split is not None else 'N/A'}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF0XlpQRUjTv"
      },
      "source": [
        "### Task 2: Discuss your findings in detail\n",
        "Provide detailed explanation and discussion about your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Most Informative Features:\n",
        "\n",
        "Calories Burned and Step Count are the most informative features, with the highest Information Gain values (8.9658 and 8.9418, respectively). This means they are the most useful for splitting the dataset and making predictions about the target variable.\n",
        "\n",
        "These features likely have a strong relationship with the target variable, making them critical for decision-making in the model.\n",
        "\n",
        "##** 2. Moderately Informative Features:**\n",
        "\n",
        "Workout Duration and Heart Rate have lower Information Gain values (6.5879 and 6.7247, respectively). While they are still useful, they are less informative compared to Calories Burned and Step Count.\n",
        "\n",
        "These features might still contribute to the model but are less critical than the top two.\n",
        "\n",
        "##3. **Best Split Value = N/A:**\n",
        "\n",
        "The N/A for Best Split Value across all features suggests that:\n",
        "\n",
        "The features might not require further splitting (e.g., they are already optimal for decision-making).\n",
        "\n",
        "The dataset might not have enough variability or clear thresholds for splitting continuous features.\n",
        "\n",
        "The features might be categorical, and no split value is needed.\n",
        "\n",
        "##4. **Implications for Model Building:**\n",
        "\n",
        "Since Calories Burned and Step Count have the highest Information Gain, they should be prioritized in the decision tree model.\n",
        "\n",
        "Workout Duration and Heart Rate can be included but might not contribute as significantly to the model's performance.\n",
        "\n",
        "If the Best Split Value is N/A, the model might rely on binary splits or other criteria for further decision-making."
      ],
      "metadata": {
        "id": "GEmFyROE2bIG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuFgLLCcUjTw"
      },
      "source": [
        "## Submission\n",
        "Submit your completed Jupyter Notebook file through the submission link in Blackboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4QsDi7fUjTw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}